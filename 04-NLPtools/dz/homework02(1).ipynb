{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "727bd75d-3dfc-4fe7-b4b6-a251628810a9",
      "metadata": {
        "tags": [],
        "id": "727bd75d-3dfc-4fe7-b4b6-a251628810a9"
      },
      "source": [
        "#### Задача 1 (10 баллов).\n",
        "\n",
        "Попробуем себя в решении задачи определения темы текста. Будем считать, что два текста похожи по теме, если у них больше общих слов (только не предлогов с союзами), чем у других текстов. У нашей программы для определения темы будет несколько готовых текстов (достаточно больших!) с уже известной темой в базе: выберите тексты (и темы) самостоятельно, 5-6 будет достаточно.\n",
        "\n",
        "Что должна делать программа? При запуске вы ей сообщаете название нового файла с текстом, который нужно классифицировать, она его открывает, обрабатывает и сравнивает с текстами в своей базе. С которым из текстов оказалось больше всего общих слов, того и тема! Очевидно, вам понадобится какие-то слова из текстов отбрасывать (подумайте, каким образом это сделать - здесь на самом деле несколько вариантов концепций), а еще лемматизировать или хотя бы использовать стемминг.\n",
        "\n",
        "Когда будете сдавать это задание, пожалуйста, пришлите и файлы с текстами. И имейте в виду, если тексты будут вставлены прямо в код и слишком короткие, я задачу засчитаю только вполовину.\n",
        "\n",
        "Напоминаю, как открываются файлы:\n",
        "\n",
        "```\n",
        "with open('путь к файлу - пишите прямые слеши', 'r', encoding='utf-8') as f:\n",
        "    text = f.read() # все содержимое вашего файла считается в одну длинную строку\n",
        "```\n",
        "\n",
        "Настоятельно советую оформить код хотя бы в функции."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7f44dfb9-10a8-401c-9e56-74a8ecb6a00c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "7f44dfb9-10a8-401c-9e56-74a8ecb6a00c",
        "outputId": "544178b6-5a85-4958-9a0d-62ec8dfcaf27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-92a6bdc66ad8>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Название файла в директории (с расширением): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_input' is not defined"
          ]
        }
      ],
      "source": [
        "# тема комиксы/художники комиксов\n",
        "#это можно было сделать гораздо красивее, но времени нет(\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sample = input(\"Название файла (с расширением): \")\n",
        "\n",
        "with open(sample, 'r', encoding='utf-8') as f:\n",
        "    text0 = f.read()\n",
        "    f.close()\n",
        "with open('1.txt', 'r', encoding='utf-8') as f:\n",
        "    text1 = f.read()\n",
        "    f.close()\n",
        "with open('2.txt', 'r', encoding='utf-8') as f:\n",
        "    text2 = f.read()\n",
        "    f.close()\n",
        "with open('3.txt', 'r', encoding='utf-8') as f:\n",
        "    text3 = f.read()\n",
        "    f.close()\n",
        "with open('4.txt', 'r', encoding='utf-8') as f:\n",
        "    text4 = f.read()\n",
        "    f.close()\n",
        "with open('5.txt', 'r', encoding='utf-8') as f:\n",
        "    text5 = f.read()\n",
        "    f.close()\n",
        "\n",
        "tokens0 = word_tokenize(text0) #токенизация всего сразу\n",
        "tokens1 = word_tokenize(text1)\n",
        "tokens2 = word_tokenize(text2)\n",
        "tokens3 = word_tokenize(text3)\n",
        "tokens4 = word_tokenize(text4)\n",
        "tokens5 = word_tokenize(text5)\n",
        "\n",
        "#на этом этапе нужно выкинуть всю пунктуацию\n",
        "\n",
        "lemma0 = lemmatizer.lemmatize(tokens0) #лемматизация всего сразу\n",
        "lemma1 = lemmatizer.lemmatize(tokens1)\n",
        "lemma2 = lemmatizer.lemmatize(tokens2)\n",
        "lemma3 = lemmatizer.lemmatize(tokens3)\n",
        "lemma4 = lemmatizer.lemmatize(tokens4)\n",
        "lemma5 = lemmatizer.lemmatize(tokens5)\n",
        "\n",
        "a = len(list(set(tokens0).intersection(set(tokens1)))) #рассматриваем размеры пересечений лемм\n",
        "b = len(list(set(tokens0).intersection(set(tokens2))))\n",
        "c = len(list(set(tokens0).intersection(set(tokens3))))\n",
        "d = len(list(set(tokens0).intersection(set(tokens4))))\n",
        "e = len(list(set(tokens0).intersection(set(tokens5))))\n",
        "\n",
        "random_list = []\n",
        "random_list.append (a,b,c,d,e)\n",
        "\n",
        "closest = max (random_list) #мы записали все размеры пересечений по порядку в массив, и теперь выбираем максимальное пересечение\n",
        "\n",
        "match closest:\n",
        "  case random_list[0]: print (\"Совпадает с текстом 1\")\n",
        "  case random_list[1]: print (\"Совпадает с текстом 2\")\n",
        "  case random_list[2]: print (\"Совпадает с текстом 3\")\n",
        "  case random_list[3]: print (\"Совпадает с текстом 4\")\n",
        "  case random_list[4]: print (\"Совпадает с текстом 5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "867f7c4b-0934-4f27-a7b0-edf84185bbf6",
      "metadata": {
        "tags": [],
        "id": "867f7c4b-0934-4f27-a7b0-edf84185bbf6"
      },
      "source": [
        "#### Задача 2 (10 баллов).\n",
        "\n",
        "Некоторые предлоги в русском языке могут управлять разными падежами (например, \"я еду в Лондон\" vs \"я живу в Лондоне\"). Давайте проанализируем эти предлоги и их падежи. Необходимо:\n",
        "\n",
        "- составить список таких предлогов (РГ-80 вам в помощь)\n",
        "- взять достаточно большой текст (можно большое художественное произведение)\n",
        "- сделать морфоразбор этого текста\n",
        "- Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога.\n",
        "\n",
        "Примечания: во-первых, имейте в виду, что иногда после предлога могут идти самые неожиданные вещи: \"я что, должен ехать на, черт побери, северный полюс?\". Во-вторых, неплохо бы учитывать отсутствие пунктуации (конечно, в норме, как нам кажется, предлог обязательно требует зависимое, но! \"да иди ты на!\") Эти штуки можно отсеять, если просто учитывать только заранее определенные падежи, а не считать все, какие встретились (так и None можно огрести).\n",
        "\n",
        "Если будете использовать RNNMorph, возможно, понадобится регулярное выражение и немного терпения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a306018-c6cd-4254-ae7e-0c43f397c128",
      "metadata": {
        "id": "9a306018-c6cd-4254-ae7e-0c43f397c128"
      },
      "outputs": [],
      "source": [
        "#этот код ещё ужаснее...\n",
        "#слова, идущие после предлогов, записываются в отдельный массив для каждого предлога. потом для каждого в цикле идёт проверка на соответствие падежу, и таким образом обновляются счётчики.\n",
        "#наверное, можно было бы обойтись одним массивом и сильно меньшим количеством строчек, но я пока это никак не оптимизировал и не делал отладку...\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pymorphy2\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "with open('222.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    f.close()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "prepositions = ['на', 'о', 'об', 'обо', 'в', 'во', 'за', 'под', 'подо', 'между', 'меж', 'по', 'с', 'со']\n",
        "\n",
        "c = nltk.ConcordanceIndex(text.tokens, key = lambda s: s.lower())\n",
        "\n",
        "i = 0\n",
        "\n",
        "nominative = 0 #пока не знаю, включать ли loc2, acc2, gen2\n",
        "genitive = 0\n",
        "dative = 0\n",
        "accusative = 0\n",
        "ablative = 0\n",
        "locative = 0\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "na = []\n",
        "na = [text1.tokens[offset+1] for offset in c.offsets(prepositions[0])]\n",
        "while i < len (na): #падежи винительный, предложный\n",
        "  parse = morph.parse(na[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'loct':\n",
        "    locative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'locative:', locative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "locative = 0\n",
        "\n",
        "\n",
        "\n",
        "o = []\n",
        "o = [text1.tokens[offset+1] for offset in c.offsets(prepositions[1])]\n",
        "while i < len (o): #падежи винительный, предложный\n",
        "  parse = morph.parse(o[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'loct':\n",
        "    locative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'locative:', locative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "locative = 0\n",
        "\n",
        "\n",
        "\n",
        "ob = []\n",
        "ob = [text1.tokens[offset+1] for offset in c.offsets(prepositions[2])]\n",
        "while i < len (ob): #падежи винительный, предложный\n",
        "  parse = morph.parse(ob[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'loct':\n",
        "    locative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'locative:', locative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "locative = 0\n",
        "\n",
        "\n",
        "\n",
        "obo = []\n",
        "obo = [text1.tokens[offset+1] for offset in c.offsets(prepositions[3])]\n",
        "while i < len (obo): #падежи винительный, предложный\n",
        "  parse = morph.parse(obo[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'loct':\n",
        "    locative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'locative:', locative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "locative = 0\n",
        "\n",
        "\n",
        "\n",
        "v = []\n",
        "v = [text1.tokens[offset+1] for offset in c.offsets(prepositions[4])]\n",
        "while i < len (v): #падежи винительный, предложный\n",
        "  parse = morph.parse(v[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'loct':\n",
        "    locative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'locative:', locative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "locative = 0\n",
        "\n",
        "\n",
        "\n",
        "vo = []\n",
        "vo = [text1.tokens[offset+1] for offset in c.offsets(prepositions[5])]\n",
        "while i < len (vo): #падежи винительный, предложный\n",
        "  parse = morph.parse(vo[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'loct':\n",
        "    locative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'locative:', locative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "locative = 0\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "za = []\n",
        "za = [text1.tokens[offset+1] for offset in c.offsets(prepositions[6])]\n",
        "while i < len (za): #падежи винительный, творительный\n",
        "  parse = morph.parse(za[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'ablt':\n",
        "    ablative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'ablative:', ablative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "ablative = 0\n",
        "\n",
        "\n",
        "\n",
        "pod = []\n",
        "pod = [text1.tokens[offset+1] for offset in c.offsets(prepositions[7])]\n",
        "while i < len (pod): #падежи винительный, творительный\n",
        "  parse = morph.parse(pod[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'ablt':\n",
        "    ablative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'ablative:', ablative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "ablative = 0\n",
        "\n",
        "\n",
        "\n",
        "podo = []\n",
        "podo = [text1.tokens[offset+1] for offset in c.offsets(prepositions[8])]\n",
        "while i < len (podo): #падежи винительный, творительный\n",
        "  parse = morph.parse(podo[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'ablt':\n",
        "    ablative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'ablative:', ablative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "ablative = 0\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "mezdu = []\n",
        "mezdu = [text1.tokens[offset+1] for offset in c.offsets(prepositions[9])]\n",
        "while i < len (mezdu): #падежи родительный, творительный\n",
        "  parse = morph.parse(mezdu[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'gent':\n",
        "    genitive+=1\n",
        "  if t.case == 'ablt':\n",
        "    ablative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('genitive:' genitive, ' ', 'ablative:', ablative, \"\\n\")\n",
        "i = 0\n",
        "genitive = 0\n",
        "ablative = 0\n",
        "\n",
        "\n",
        "\n",
        "mez = []\n",
        "mez = [text1.tokens[offset+1] for offset in c.offsets(prepositions[10])]\n",
        "while i < len (mez): #падежи родительный, творительный\n",
        "  parse = morph.parse(mez[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'gent':\n",
        "    genitive+=1\n",
        "  if t.case == 'ablt':\n",
        "    ablative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('genitive:' genitive, ' ', 'ablative:', ablative, \"\\n\")\n",
        "i = 0\n",
        "genitive = 0\n",
        "ablative = 0\n",
        "\n",
        "\n",
        "\n",
        "s = []\n",
        "s = [text1.tokens[offset+1] for offset in c.offsets(prepositions[12])]\n",
        "while i < len (s): #падежи родительный, творительный\n",
        "  parse = morph.parse(s[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'gent':\n",
        "    genitive+=1\n",
        "  if t.case == 'ablt':\n",
        "    ablative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('genitive:' genitive, ' ', 'ablative:', ablative, \"\\n\")\n",
        "i = 0\n",
        "genitive = 0\n",
        "ablative = 0\n",
        "\n",
        "\n",
        "\n",
        "so = []\n",
        "so = [text1.tokens[offset+1] for offset in c.offsets(prepositions[13])]\n",
        "while i < len (so): #падежи родительный, творительный\n",
        "  parse = morph.parse(so[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'gent':\n",
        "    genitive+=1\n",
        "  if t.case == 'ablt':\n",
        "    ablative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('genitive:' genitive, ' ', 'ablative:', ablative, \"\\n\")\n",
        "i = 0\n",
        "genitive = 0\n",
        "ablative = 0\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "po = []\n",
        "po = [text1.tokens[offset+1] for offset in c.offsets(prepositions[11])]\n",
        "while i < len (po): #падежи винительный, предложный, дательный\n",
        "  parse = morph.parse(po[i])\n",
        "  t = parse[0].tag\n",
        "  if t.case == 'acc':\n",
        "    accusative+=1\n",
        "  if t.case == 'loct':\n",
        "    locative+=1\n",
        "  if t.case == 'datv':\n",
        "    dative+=1\n",
        "\n",
        "  i+=1\n",
        "print ('accusative:' accusative, ' ', 'locative:', locative, ' ', 'dative:', dative, \"\\n\")\n",
        "i = 0\n",
        "accusative = 0\n",
        "locative = 0\n",
        "dative = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f8c885-9e8f-4bbc-a0ab-ac801af64245",
      "metadata": {
        "id": "12f8c885-9e8f-4bbc-a0ab-ac801af64245"
      },
      "source": [
        "\n",
        "#### Задача 3 (5 баллов).\n",
        "\n",
        "Представим, что у вас есть файл с разборами conllu (можете взять любой, например, [тут](https://github.com/dialogue-evaluation/GramEval2020)). Нужно просмотреть все примеры предложений с тегом dislocated и тегом discourse: напишите скрипт, который будет читать файл, находить все такие предложения и\n",
        "печатать: 1) сам текст предложения 2) слово, имеющее искомый тег. Если тег не был найден в файле, нужно об этом сообщить. Постарайтесь оформить вывод таким образом, чтобы это было удобно читать.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ef478943-1842-4dfe-98dc-327860a907bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "ef478943-1842-4dfe-98dc-327860a907bf",
        "outputId": "07a640f0-26fe-4990-8e3f-66601731a424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyconll\n",
            "  Downloading pyconll-3.2.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.2.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-74c334424d53>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyconll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GramEval2020-GSD-train.conllu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyconll/load.py\u001b[0m in \u001b[0;36mload_from_file\u001b[0;34m(file_descriptor)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mParseError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m \u001b[0mparsing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mConll\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GramEval2020-GSD-train.conllu'"
          ]
        }
      ],
      "source": [
        "!pip install pyconll\n",
        "import pyconll\n",
        "import re\n",
        "import pymorphy2\n",
        "\n",
        "text = pyconll.load_from_file('GramEval2020-GSD-train.conllu')\n",
        "splits = text.split (\"\\n\\n\") #распиливает строку на предложения, смотря по двойному отступу\n",
        "print (len(splits)) #смотрим сколько элементов\n",
        "i = 0\n",
        "\n",
        "while i < (len(splits)):\n",
        "  if 'discourse' or 'dislocated' in splits[i]:\n",
        "    current = parse (splits[i])\n",
        "    current\n",
        "  else:\n",
        "    print ('Not found\\n')\n",
        "\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd0db77-e073-40ff-8c7d-3512325ce991",
      "metadata": {
        "id": "abd0db77-e073-40ff-8c7d-3512325ce991"
      },
      "source": [
        "#### Задача 4 (5 баллов).\n",
        "\n",
        "Возьмите любой достаточно длинный (лучше новостной) текст. Любым известным инструментом извлеките именованные сущности из этого текста и выведите их списком по категориям (т.е. персоны вместе, локации вместе, организации вместе)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0ff0b9-6bca-42a7-baea-f2c856e6059f",
      "metadata": {
        "id": "4b0ff0b9-6bca-42a7-baea-f2c856e6059f"
      },
      "outputs": [],
      "source": [
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "jar = \"stanford-ner-2015-04-20/stanford-ner-3.5.2.jar\"\n",
        "model = \"stanford-ner-2015-04-20/classifiers/\"\n",
        "st_4class = StanfordNERTagger(model + \"english.conll.4class.distsim.crf.ser.gz\", jar, encoding='utf8')  #используется 4 class model for recognizing locations, persons, organizations, and miscellaneous entities\n",
        "\n",
        "with open('news.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    f.close()\n",
        "\n",
        "for i in [st_4class.tag(text.split())]:\n",
        "  for b in i:\n",
        "    if b[1] != 'O':\n",
        "        print(b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4762e8d-baca-4d22-b6f3-3866fee70a7d",
      "metadata": {
        "id": "e4762e8d-baca-4d22-b6f3-3866fee70a7d"
      },
      "source": [
        "#### Задача на бонусные 5 баллов:\n",
        "\n",
        "Сравните качество несколькиз разных морфопарсеров для любого языка, где их больше одного. Разберите этими морфопарсерами один и тот же текст, если они все разбирают в UD, можете вывести автоматически расхождения, в противном случае просмотрите глазами на наличие ошибок (текст, конечно, слишком большой лучше не брать).\n",
        "\n",
        "Без письменных выводов-комментариев не засчитывается."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89279821-5d4f-428e-b61c-33028a3d1374",
      "metadata": {
        "id": "89279821-5d4f-428e-b61c-33028a3d1374"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}